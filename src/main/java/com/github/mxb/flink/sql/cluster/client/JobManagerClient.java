package com.github.mxb.flink.sql.cluster.client;import com.alibaba.fastjson.JSONObject;import com.github.mxb.flink.sql.cluster.AbstractClusterClient;import com.github.mxb.flink.sql.exception.FlinkClientTimeoutException;import com.github.mxb.flink.sql.cluster.client.flinkcli.FlinkCliFrontend;import com.github.mxb.flink.sql.cluster.model.run.overview.JobRunOverview;import com.github.mxb.flink.sql.cluster.model.run.overview.JobRunPathEnum;import com.github.mxb.flink.sql.cluster.model.run.overview.JobRunStatusEnum;import com.github.mxb.flink.sql.cluster.model.run.overview.JobsRunStatus;import com.github.mxb.flink.sql.cluster.model.run.JobConfig;import com.github.mxb.flink.sql.cluster.config.JobManagerConfiguration;import com.github.mxb.flink.sql.util.OkHttpUtils;import com.github.mxb.flink.sql.cluster.model.run.JobRunConfig;import com.github.mxb.flink.sql.cluster.model.run.ProgramResultDescriptor;import com.github.mxb.flink.sql.util.JarUtils;import okhttp3.Response;import org.apache.commons.cli.Options;import org.apache.flink.client.cli.CliFrontendParser;import org.apache.flink.client.cli.CustomCommandLine;import org.apache.flink.configuration.Configuration;import org.apache.flink.configuration.RestOptions;import org.apache.flink.sql.parser.error.SqlParseException;import org.apache.flink.table.client.SqlClientException;import org.apache.flink.table.client.gateway.ProgramTargetDescriptor;import org.apache.flink.table.client.gateway.SqlExecutionException;import org.apache.flink.util.FlinkException;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.File;import java.io.IOException;import java.text.MessageFormat;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.Objects;/** * @description     TODO * @auther          moxianbin * @create          2020-04-11 19:11:23 */public class JobManagerClient<T> extends AbstractClusterClient<T> {    private static final Logger LOG = LoggerFactory.getLogger(JobManagerClient.class);    private Configuration flinkConfig;    private List<CustomCommandLine<?>> commandLines;    private Options commandLineOptions;    //JobManagerConfiguration    private JobManagerConfiguration jobManagerConfiguration;    public JobManagerClient(){}    public JobManagerClient(Configuration configuration){        this.jobManagerConfiguration = JobManagerConfiguration.builder()                .jobManageAddress(configuration.getString(RestOptions.ADDRESS))                .jobManageRestPort(configuration.getInteger(RestOptions.PORT))                .build();        // init the configuration        Configuration config = new Configuration();        config.setString("rest.port", String.valueOf(jobManagerConfiguration.getJobManageRestPort()));        config.setString("jobmanager.rpc.address", jobManagerConfiguration.getJobManageAddress());        try {            this.flinkConfig = config;            // load command lines for deployment            this.commandLines = FlinkCliFrontend.loadCustomCommandLines(flinkConfig);            this.commandLineOptions = collectCommandLineOptions(commandLines);        } catch (Exception e) {            throw new SqlClientException("Could not init Flink configuration.", e);        }    }    @Override    public String cancel(String jobId, String savepointDir) throws FlinkException, FlinkClientTimeoutException {        String jobManageUrl = jobManagerConfiguration.getJobManageAddress() + ":" + jobManagerConfiguration.getJobManageRestPort();        String savepointPath = null;        try {            savepointPath = FlinkCliFrontend.cancelHandle(jobManageUrl, jobId, savepointDir);        } catch (Exception e) {            reThrowException(e);        }        return savepointPath;    }    @Override    public String stop(String jobId, String savepointDir) throws FlinkException, FlinkClientTimeoutException {        String jobManageUrl = jobManagerConfiguration.getJobManageAddress() + ":" + jobManagerConfiguration.getJobManageRestPort();        String savepointPath = null;        try {            savepointPath = FlinkCliFrontend.stopHandle(jobManageUrl, jobId, savepointDir);        } catch (Exception e) {            reThrowException(e);        }        return savepointPath;    }    @Override    public String triggerSavepoint(String jobId, String savepointDir) throws FlinkException, FlinkClientTimeoutException {        String jobManageUrl = jobManagerConfiguration.getJobManageAddress() + ":" + jobManagerConfiguration.getJobManageRestPort();        String savepointPath = null;        try {            savepointPath = FlinkCliFrontend.savepointHandle(jobManageUrl, jobId, savepointDir);        } catch (Exception e) {            reThrowException(e);        }        return savepointPath;    }    @Override    public ProgramTargetDescriptor executeSqlJob(JobRunConfig jobRunConfig, String dependencyJarDir, String sql) throws SqlExecutionException, SqlParseException, FlinkClientTimeoutException {        List<File> dependencyJars = JarUtils.getJars(dependencyJarDir);        JobConfig jobConfig = new JobConfig(jobRunConfig, new HashMap<>());        ProgramTargetDescriptor programTargetDescriptor = null;        try{            programTargetDescriptor = executeSqlJob(jobConfig, dependencyJars, sql, flinkConfig, commandLineOptions, commandLines);        } catch (Exception e){            if ( isConnectionProblemException().test(e) ){                throw new FlinkClientTimeoutException(e);            }            throw e;        }        return programTargetDescriptor;    }    @Override    public ProgramTargetDescriptor executeSqlJob(JobRunConfig jobRunConfig, List<File> dependencyJars, String sql) throws FlinkException, SqlExecutionException, SqlParseException, FlinkClientTimeoutException {        JobConfig jobConfig = new JobConfig(jobRunConfig, new HashMap<>());        ProgramTargetDescriptor programTargetDescriptor;        try{            programTargetDescriptor = executeSqlJob(jobConfig, dependencyJars, sql, flinkConfig, commandLineOptions, commandLines);        }catch (Exception e) {            if ( isConnectionProblemException().test(e) ){                throw new FlinkClientTimeoutException(e);            }            throw e;        }        return programTargetDescriptor;    }    @Override    public ProgramTargetDescriptor executeSqlJob(JobConfig jobConfig, List<File> dependencyJars, String sql) throws FlinkException, SqlExecutionException, SqlParseException, FlinkClientTimeoutException {        ProgramTargetDescriptor programTargetDescriptor;        try {            programTargetDescriptor = executeSqlJob(jobConfig, dependencyJars, sql, flinkConfig, commandLineOptions, commandLines);        } catch (Exception e) {            if ( isConnectionProblemException().test(e) ){                throw new FlinkClientTimeoutException(e);            }            throw e;        }        return programTargetDescriptor;    }    @Override    public Map<String, JobRunStatusEnum> getJobStatus(List<String> jobIds) throws FlinkException, FlinkClientTimeoutException {        Map<String,String> jobRunStatus =  getJobsRunStatus(jobIds);        Map<String, JobRunStatusEnum> jobRunStatusEnumMap = new HashMap<>();        jobRunStatus.forEach((jobId, jobRunStatusStr) -> {            try {                jobRunStatusEnumMap.put(jobId, JobRunStatusEnum.valueOf(jobRunStatusStr));            } catch (Exception e){                LOG.error("查询jobStatus发现有未配置的状态,jobId:{},jobStatus:{}", jobId, jobRunStatusStr, e);            }        });        return jobRunStatusEnumMap;    }    @Override    public Map<String, JobRunOverview> getJobsOverview(List<String> jobIds) throws FlinkClientTimeoutException, FlinkException {        Map<String, JobRunOverview> overviewMap = new HashMap<>();        for (String jobId : jobIds) {            String jobOverviewPath = JobRunPathEnum.JOB_OVERVIEW.getPath().replace("{jobId}", jobId);            try {                JobRunOverview jobRunOverview = getInfoByFlinkRest(jobOverviewPath, JobRunOverview.class);                overviewMap.put(jobId, jobRunOverview);            } catch (IOException e) {                reThrowException(e);            }        }        return overviewMap;    }    @Override    public Map<String, String> getJobsOverviewUrl(List<String> jobIds) {        Map<String, String> overviewUrlMap = new HashMap<>();        jobIds.forEach(jobId->{            String overviewUrl = MessageFormat.format("http://{0}:{1}/#/job/{2}/overview",                    jobManagerConfiguration.getJobManageAddress(),String.valueOf(jobManagerConfiguration.getJobManageRestPort()),jobId);            overviewUrlMap.put(jobId, overviewUrl);        });        return overviewUrlMap;    }    @Override    public ProgramResultDescriptor executeQueryInternal(JobRunConfig jobRunConfig, String dependencyJarDir, String sql) throws SqlExecutionException, SqlParseException {        throw new SqlExecutionException("Unsupported");    }    // -----monitor--------------------------------------------------------------------------------------------    private static <T> T getType(Response response, Class<T> cls) throws IOException {        if (response.isSuccessful()) {            if (response.body() == null) {return null;}            if (cls == String.class) {return (T)response.body().string();}            String content = response.body().string();            return JSONObject.parseObject(content, cls);        }        return null;    }    private <T> T getInfoByFlinkRest(String reqPath, Class<T> cls) throws IOException {        final String host = jobManagerConfiguration.getJobManageAddress();        final Integer rpcPort = jobManagerConfiguration.getJobManageRestPort();        String url = "";        if (Objects.nonNull(rpcPort)) {            url = MessageFormat.format("http://{0}:{1}/{2}", String.valueOf(host), String.valueOf(rpcPort), reqPath);        } else {            url = MessageFormat.format("http://{0}/{1}", String.valueOf(host), reqPath);        }        Response resp = OkHttpUtils.get(url);        return getType(resp, cls);    }    private Map<String, String> getJobsRunStatus(List<String> jobIds) throws FlinkException, FlinkClientTimeoutException {        //filter none job        Map<String,String> jobRunStatusResultMap = new HashMap<>();        try {            JobsRunStatus flinkJobsRunStatus = getInfoByFlinkRest(JobRunPathEnum.JM_JOBS.getPath(), JobsRunStatus.class);            jobIds.forEach(jobId->{                if (!flinkJobsRunStatus.contains(jobId)){                    jobRunStatusResultMap.put(jobId, JobRunStatusEnum.NONE.name());                }            });            //fill jobsStatus            flinkJobsRunStatus.getJobs().forEach(jobsInfo -> {                if (jobIds.contains(jobsInfo.getId())){                    jobRunStatusResultMap.put(jobsInfo.getId(), jobsInfo.getStatus());                }            });        } catch (Exception e){            reThrowException(e);        }        return jobRunStatusResultMap;    }    // --------------------------------------------------------------------------------------------    private static Options collectCommandLineOptions(List<CustomCommandLine<?>> commandLines) {        final Options customOptions = new Options();        for (CustomCommandLine<?> customCommandLine : commandLines) {            customCommandLine.addRunOptions(customOptions);        }        return CliFrontendParser.mergeOptions(                CliFrontendParser.getRunCommandOptions(),                customOptions);    }}